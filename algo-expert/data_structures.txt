Data Structures

- Coding is manipulating data to accomplish something.
- Data structures are a way to organize and manage data. 
- A data structure is a collection of data values, the 
relationships among them, and the functions or operations 
that can be applied to the data. 

Complexity Analysis 

- a problem can have multiple solutions, but some are better than others
- the bedrock of coding interviews
- time complexity
	- measure of how fast an algorithm runs
- space complexity 
	- how much memory or space an algorithm uses up 
- referred together as space-time complexity
- different data structures are going to have different
time and space complexity ramifications

Memory

- why is space important? memory is not unlimited. 
- comprised of bits, 0s and 1s. 8 bits = byte.
- byte e.g. 0000 0001 
- this is (2^7 2^6 2^5 2^4  2^3 2^2 2^1 2^0) binary number format (base-2 format)
- 256 possible values in a byte
- how to store more than value 256? increase the number of bits
that are stored in a data value. for 16- or 32- or 64-bit 
architecture 
- can point to another memory address from current memory address.
this is a pointer.
- a list of items needs to be stored back-to-back in memory slots
that must be free

Big O Notation

Time complexity: measuring the change in speed of the algorithm with respect to the size of the input. 
Asymptotic analysis: study the behavior of a function f(n) as the value n moves towards infinity. 

Best to worst complexity (given the worst-case scenario): 
O(1) Constant Time. Meaning N/space is irrelevant to performance.
O(log(N))
O(N) Linear Time Complexity. As size of N input increases, the speed of the algorithm increases linearly. Every single element traversed through.
O(2N) = O(N) 
O(N log(N)) 
O(N^2).    Every single element traversed through twice. 
O(N^3).
O(N^4 + N^2) = O(N^4) 
O(2^N) 
O(N!) // N factorial 

note: if have two inputs: O(N + M) for example. 

Logarithm 

log sub b (x) = y iif b^y = x 

- the log of the number X given a base b is equal to y 
"aka what is the exponent/power needed to get x value if starting with b value"
if and only if 
b to the power of y is equal to x 

we always assume that we are dealing with base 2 unless otherwise specified.
binary logarithm.
e.g.
read "log base 2 of (N) is equal to y iif 2^y = N"
because we can assume: "log of N equals y if and only if 2 to the power of y equals N"

whenever you increase the y exponent, you are doubling N (given base 2) 
e.g. 2^2 = 4, 2^3 = 8....

--> This means as the input increases, as the input doubles, the number y of elementary
operations we are performing in the algorithm only increases by 1. 
A log(N) complexity is much better than linear complexity which would increase by 
N operations as the input increases.

--> As the input doubles in size, we only do one additional operation. This is 
a time complexity of log(N) "log of N". 


Strings

- differ a bit given programming language
- a string is stored in memory as an array of integers where each character is mapped 
to an integer using an encoding standard like ASCII, e.g. A => 65.
- traversing a string is O(N) in Time Complexity and O(1) Space Complexity operation
- copy a string is O(N) in space time complexity
- getting a string is O(1) in space time complexity 


Stacks and Queues

Stack. Think of a book stack. Stacking a book on top and then removing it would mean
the book is last in, first out LIFO. 

Queue. Think of people waiting in line to order coffee. The first person in line will
be the first person to order coffee. First in, first out. FIFO.

Insertions and deletions in stacks and queues run in constant space time O(1)
Searches are O(N) time and constant space O(1) 
To store data O(N) space


Hash Tables

- A key/value store 
- Searching for a value, inserting, and deleting given a key runs on constant time O(1)
  on average 
- under the hood of a hash table is an array. a key, a string, is transformed by a hash 
  function into a index.  
- the hash function find the ascii value for each character in a string e.g. "foo" and
  adds them up. e.g. 301. This number is used to find the index using the modulo operator
  and the length of the underlying array (hash table). e.g. if the array is length 3, then
  301 % 3. This is equal to 1. So the key "foo" would be transformed into index 1. 
- what happens if two or more keys are transformed into the same index? then you get 
  a linked list. e.g. given a hash table:

  "abc" => 1  // goes through hash function: 302 % 3 === 2
  "def" => 2  // goes through hash function: 302 % 3 === 2
  "ghi" => 3  // goes through hash function: 300 % 3 === 0

            [ ,  ,  ]
   "ghi" <-- 3	   1 --> "abc"
                   |		// this is a linked list
                   +
	           2 --> "def" 

- Searching for a value, inserting, and deleting given a key runs on constant time O(N)
  on worst case. That is if all the key values have the same index under the hood, 
  then you have a linked list the same length as the hash table. 


Arrays 

64 bit integers take 8 bytes, 8 'memory slots'. 
Arrays require contiguous memory slots.

So storing an array like [1, 2, 3] would take up 8 x 3 = 24 memory slots. 
This would be a static array, an array with an exact length, in this case, 24. 

get is O(1) space time
set is O(1) space time
initialize an array is O(N) space time
copying an array is O(N) space time!
traverse is O(N) time and constant space O(1)

Dynamic arrays (can change in length) are standard in JavaScript and Python.
Generally when an array is initialized it will be the length of the initialization 
plus approximately double the length. This creates 'free slots' and thus allows for
quick insertions. Once the end of the array filled up, then the array is copied and the
length of the filled up array is added to the end of the copied array. 

For dynamic arrays, std arrays in JS and Python: 

Inserting at the end of an array is O(1) constant time and O(1) space. 
Inserting at the beginning or middle of an array is O(N) space time. 

In JS and Python when removing the first item of an array, the time is O(N) because
you are shifting all of the elements in the array. 


Linked Lists 

Is a popular data structure for being reversed.
Elements are connected using pointers. 
e.g. memory slot 1 points to memory slot 20, etc.
This is unlike arrays which requires contiguous memory slots. 

Node --> Node --> Node 

Each node has a value and a pointer to the next node in a linked list. 
The first node is the 'head.' Last node is the 'tail.'

You have to traverse an entire linked list to get to a value at index x. 

Getting a node: O(i) time and O(1) space complexity. That is, i is the 'index'
Setting a node: O(i) time and O(1) space.

Initializing a linked list: O(N) space time
Copying a linked list: O(N) space time

Insertion - O(N) space time 
Deletion - O(N) space time 


Graphs

A graph is a collection of nodes that may or may not be related to one another.

Nodes = vertices
Connections = edges

Nodes may have values e.g. integer values.

A graph is 'connected' if any two nodes can reach each other.

Note: some graphs are directive meaning the edges in the graph have directions.

e.g. (Node A) ---> (Node B) <---> (Node C) 

A cycle in the graph is when you can traverse 3 nodes in a circular direction.
e.g. A to B to C to A. 

Storing a graph is O(V + E) space. V being vertices and E edges.
























































