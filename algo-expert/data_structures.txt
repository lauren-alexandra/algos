Data Structures

- Coding is manipulating data to accomplish something.
- Data structures are a way to organize and manage data. 
- A data structure is a collection of data values, the 
relationships among them, and the functions or operations 
that can be applied to the data. 

Complexity Analysis 

- a problem can have multiple solutions, but some are better than others
- the bedrock of coding interviews
- time complexity
	- measure of how fast an algorithm runs
- space complexity 
	- how much memory or space an algorithm uses up 
- referred together as space-time complexity
- different data structures are going to have different
time and space complexity ramifications

Memory

- why is space important? memory is not unlimited. 
- comprised of bits, 0s and 1s. 8 bits = byte.
- byte e.g. 0000 0001 
- this is (2^7 2^6 2^5 2^4  2^3 2^2 2^1 2^0) binary number format (base-2 format)
- 256 possible values in a byte
- how to store more than value 256? increase the number of bits
that are stored in a data value. for 16- or 32- or 64-bit 
architecture 
- can point to another memory address from current memory address.
this is a pointer.
- a list of items needs to be stored back-to-back in memory slots
that must be free

Big O Notation

Time complexity: measuring the change in speed of the algorithm with respect to the size of the input. 
Asymptotic analysis: study the behavior of a function f(n) as the value n moves towards infinity. 

Best to worst complexity (given the worst-case scenario): 
O(1) Constant Time. Meaning N/space is irrelevant to performance.
O(log(N))
O(N) Linear Time Complexity. As size of N input increases, the speed of the algorithm increases linearly. Every single element traversed through.
O(2N) = O(N) 
O(N log(N)) 
O(N^2).    Every single element traversed through twice. 
O(N^3).
O(N^4 + N^2) = O(N^4) 
O(2^N) 
O(N!) // N factorial 

note: if have two inputs: O(N + M) for example. 

Logarithm 

log sub b (x) = y iif b^y = x 

- the log of the number X given a base b is equal to y 
"aka what is the exponent/power needed to get x value if starting with b value"
if and only if 
b to the power of y is equal to x 

we always assume that we are dealing with base 2 unless otherwise specified.
binary logarithm.
e.g.
read "log base 2 of (N) is equal to y iif 2^y = N"
because we can assume: "log of N equals y if and only if 2 to the power of y equals N"

whenever you increase the y exponent, you are doubling N (given base 2) 
e.g. 2^2 = 4, 2^3 = 8....

--> This means as the input increases, as the input doubles, the number y of elementary
operations we are performing in the algorithm only increases by 1. 
A log(N) complexity is much better than linear complexity which would increase by 
N operations as the input increases.

--> As the input doubles in size, we only do one additional operation. This is 
a time complexity of log(N) "log of N". 


Strings

- differ a bit given programming language
- a string is stored in memory as an array of integers where each character is mapped 
to an integer using an encoding standard like ASCII, e.g. A => 65.
- traversing a string is O(N) in Time Complexity and O(1) Space Complexity operation
- copy a string is O(N) in space time complexity
- getting a string is O(1) in space time complexity 



